{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gutenberg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Gutenberg was a visionary project launched by Michael Hart to create free electronic versions of literary works and disseminate them worldwide. He wanted everyone to have a digital library at no cost. Project Gutenberg got its first boost with the invention of the web in 1990, and its second boost with the creation of distributed proofreaders in 2000. Distributed proof-readers are a web-based project that supports the development of e-texts. This helped project Gutenberg by allowing many people to work together in proofreading drafts of e-texts for errors. In 2010 Project Gutenberg offered more than 33,000 eBooks being downloaded by Tens of thousands every day.([Source: Project Gutennurg](https://www.gutenbergnews.org/)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams & Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to sequences of two or three items (typically words or characters) in a text, A bigram is the instance of two multi-word tokens, which is two words that have a distinct meaning when used together. Trigrams are the same thing except with three words that are used together to mean something specific. Example \"The cat sat on the mat.\"\n",
    "Bigrams:\t\t\n",
    "\"The cat sat\"\t\n",
    "\"cat sat on\"\t\n",
    "\"sat on the\"\t\n",
    "\"on the mat\"\n",
    "\n",
    "Trigrams:\t\t    \n",
    "\"The cat\"\n",
    "\"cat sat\"\n",
    "\"sat on\"\t\n",
    "\"on the\"\n",
    " \"the mat\"\t\n",
    " \t\t\t\n",
    "These dont necessarily carry distinct meanings beyond the meanings of the individual words. However, you could have cases where bigrams or trigrams form meaningful phrases, like\n",
    "\n",
    "Bigrams:\t\t\n",
    "\"New York\"\t\t\n",
    "\"ice cream\"\t\t\n",
    "\"United States\"\n",
    "\n",
    "Trigrams:\n",
    "\"New York City\"\n",
    "\"The United States\"\n",
    "\"San Francisco Bay\"\n",
    "\n",
    "Here, some of these are indeed fixed phrases (e.g., \"New York City\") that have a specific meaning when used together, while others are just sequences of words. \"Examples of bigrams and trigrams were generated with the help of an AI language model (OpenAI's ChatGPT).\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research of Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 is asking me to buid a trigram model using 5 texts from project gutenburg.\n",
    "1. Pick 5 books from project gutenburg\n",
    "2. Clean up the text from each book\n",
    "    - Remove uneccessary parts like   introductions and footnotes\n",
    "    - Keep only letters (A-Z),periods,  (.), and spaces.\n",
    "    - Make all letters uppercase\n",
    "3. Create trigrams from the text \n",
    "    - Trigam is any sequence of three charcters like (\"THE\" OR \"E\")\n",
    "    - Go through each text, find every set of three characters, and count how many times each one appears. \n",
    "4. Store the results so it shows how each trigranm and how often it shows up.\n",
    "\n",
    "The end result will be a list of the most common sets of trigrams that appear in the text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"re\" Used for regular expressions to clean the text.\n",
    "- defaultdict from collections: Helps in counting trigrams by providing default values for keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Successfully read the file 'texts/TheGoldenRule.txt'.\n",
      "ï»¿The Project Gutenberg eBook of The golden rule\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Tit\n"
     ]
    }
   ],
   "source": [
    "def read_text(file_path):\n",
    "    \n",
    "    #Reads the content of a text file and returns it as a string.\n",
    "   \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Example usage \n",
    "file_path = 'texts/TheGoldenRule.txt'\n",
    "text = read_text(file_path)\n",
    "print(f\"Debug: Successfully read the file '{file_path}'.\")\n",
    "print(text[:500])  # Print the first 500 characters to verify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The read_text function opens a text file, reads its, and returns it as a string.\n",
    "- The with statement ensures that the file is properly closed after reading.\n",
    "- Example usage is printing the first 500 letters of the text to see if it works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE PROJECT GUTENBERG EBOOK OF THE GOLDEN RULE    THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES ANDMOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONSWHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMSOF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINEAT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATESYOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATEDBEFORE USING THIS EBOOK.TITLE THE GOLDEN \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by:\n",
    "    - Removing non-ASCII characters\n",
    "    - Keeping only letters, full stops, and spaces\n",
    "    - Converting all letters to uppercase\n",
    "    \"\"\"\n",
    "    # Remove non-ASCII characters and keep only letters, spaces, and periods.\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z. ]', '', text)\n",
    "    # Convert all letters to uppercase.\n",
    "    cleaned_text = cleaned_text.upper()\n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "cleaned_text = clean_text(text)\n",
    "print(cleaned_text[:500])  # Print the first 500 characters to test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- re.sub(r'[^a-zA-Z. ]', '', text): Removes all characters except for letters, periods, and spaces.\n",
    "- cleaned_text.upper(): Converts the entire text to uppercase, keeping everything the same.\n",
    "- This step prepares the text by removing unwanted characters and normalizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('THE', 707), ('HE ', 707), ('E P', 94), (' PR', 166), ('PRO', 140), ('ROJ', 88), ('OJE', 88), ('JEC', 89), ('ECT', 158), ('CT ', 96)]\n"
     ]
    }
   ],
   "source": [
    "def generate_trigrams(text):\n",
    "    \n",
    "    #Generates a dictionary of trigrams and their counts.\n",
    "    \n",
    "    trigrams = defaultdict(int)  \n",
    "    \n",
    "    # Loop through the text to extract trigrams.\n",
    "    for i in range(len(text) - 2):\n",
    "        trigram = text[i:i + 3]\n",
    "        trigrams[trigram] += 1\n",
    "    \n",
    "    return trigrams\n",
    "\n",
    "# Example usage\n",
    "trigram_counts = generate_trigrams(cleaned_text)\n",
    "print(list(trigram_counts.items())[:10])  # Print the first 10 trigrams to verify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- defaultdict(int): Initializes the dictionary with a default value of 0 for each trigram.\n",
    "- for i in range(len(text) - 2): Iterates through the text so we can extract three characters at a time.\n",
    "- text[i:i + 3]: Extracts a sequence of three characters.\n",
    "- trigrams[trigram] += 1: Increments the count for each trigram.\n",
    "- This step allows us to count the occurrence of each trigram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram: '   ' | Count: 777\n",
      "Trigram: ' TH' | Count: 754\n",
      "Trigram: 'THE' | Count: 707\n",
      "Trigram: 'HE ' | Count: 707\n",
      "Trigram: ' AN' | Count: 379\n",
      "Trigram: 'ED ' | Count: 360\n",
      "Trigram: 'ND ' | Count: 360\n",
      "Trigram: ' TO' | Count: 360\n",
      "Trigram: 'AND' | Count: 335\n",
      "Trigram: 'TO ' | Count: 324\n"
     ]
    }
   ],
   "source": [
    "def display_trigrams(trigrams, top_n=10):\n",
    "    \"\"\"\n",
    "    Displays the top Number(10) of trigrams sorted by their frequency.\n",
    "    \"\"\"\n",
    "    # Sort trigrams by count in descending order and get the top number.\n",
    "    sorted_trigrams = sorted(trigrams.items(), key=lambda x: x[1], reverse=True)\n",
    "    for trigram, count in sorted_trigrams[:top_n]:\n",
    "        print(f\"Trigram: '{trigram}' | Count: {count}\")\n",
    "\n",
    "# Example usage\n",
    "display_trigrams(trigram_counts, top_n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sorted(trigrams.items(), key=lambda x: x[1], reverse=True): Sorts the trigram based on their frequency in descending order.\n",
    "- for trigram, count in sorted_trigrams[:top_n]: Loops through the sorted list and prints the most common trigrams.\n",
    "- This helps us understand the most frequently occurring trigrams in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model saved to trigram.txt.\n"
     ]
    }
   ],
   "source": [
    "# Paths to the text files from Project Gutenberg.\n",
    "file_paths = ['texts/AldythsInheritance.txt', 'texts/LordListerNo.0312.txt', 'texts/NoPlaceLikeHome.txt', \n",
    "'texts/TheGoldenRule.txt', 'texts/ThewonderfulChristmasInPumpkinDelightLane.txt']\n",
    "\n",
    "# Combined trigram counts across all texts.\n",
    "combined_trigram_counts = defaultdict(int)\n",
    "\n",
    "# Process each file.\n",
    "for path in file_paths:\n",
    "    text = read_text(path)\n",
    "    cleaned_text = clean_text(text)\n",
    "    trigram_counts = generate_trigrams(cleaned_text)\n",
    "    \n",
    "    # Merge the trigram counts into the combined dictionary.\n",
    "    for trigram, count in trigram_counts.items():\n",
    "        combined_trigram_counts[trigram] += count\n",
    "\n",
    "# Display the top 10 trigrams across all texts.\n",
    "#display_trigrams(combined_trigram_counts, top_n=10)\n",
    "\n",
    "# Save the trigram model to a text file\n",
    "def save_trigram_model(trigrams, output_file):\n",
    "    \n",
    "    with open(output_file, 'w') as file:\n",
    "        for trigram, count in trigrams.items():\n",
    "            file.write(f\"{trigram}: {count}\\n\")\n",
    "\n",
    "# Save to file after processing all texts\n",
    "output_file = 'trigram.txt'\n",
    "save_trigram_model(combined_trigram_counts, output_file)\n",
    "print(f\"Trigram model saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This loop reads them, cleans them, generates trigram counts, and then merges them into a combined trigram dictionary.\n",
    "- Displays the most common trigrams across all texts in a text file called 'trigram.txt'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research of Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Task 2 is asking me to use the trigram method from Task 1 to generate a 10,000-character string by starting with \"TH\" and predicting the next character based on the previous two, By using the trigram counts to help with selection.\n",
    "\n",
    "1. Use the trigram model from task 1\n",
    "\n",
    "2. Start with \"TH\":\n",
    "    - Start generating the string using the initial characters \"TH\".\n",
    "\n",
    "3. Find trigrams that begin with last two characters:\n",
    "    -  Use the last two characters to look up matching trigrams in the model.\n",
    "\n",
    "4. Randomly select the next character:\n",
    "    - Pick the third character based on the trigram counts (higher counts = higher chance of selection).\n",
    "\n",
    "5. Update the string:\n",
    "    - Add the selected character to the string and continue the process using the updated last two characters.\n",
    "\n",
    "6. Save to the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def weighted_random_choice(trigrams, prefix):\n",
    "    \n",
    "    candidates = {k: v for k, v in trigrams.items() if k.startswith(prefix)}\n",
    "    \n",
    "    if not candidates:\n",
    "        # If no matching trigram is found, return a space\n",
    "        return ' '\n",
    "    \n",
    "    # Get the possible next characters and their counts\n",
    "    next_chars = [trigram[2] for trigram in candidates.keys()]\n",
    "    weights = list(candidates.values())\n",
    "    \n",
    "    # Randomly select the next character based on frequency\n",
    "    return random.choices(next_chars, weights=weights)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The weighted_random_choice function takes the trigram model and two-character prefix, and chooses the third character based on the trigram counts as probabilities. \n",
    "- If no matching trigram is found, it returns a space (' '). \n",
    "- Uses `random.choices()` function to perform the weighted random selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(trigrams, length=10000, start='TH'):\n",
    "   \n",
    "    text = start\n",
    "    \n",
    "    # Generate characters until it hits 10,000 characters\n",
    "    while len(text) < length:\n",
    "        # Get the last two characters\n",
    "        prefix = text[-2:]\n",
    "        # Get the next character using the weighted random choice\n",
    "        next_char = weighted_random_choice(trigrams, prefix)\n",
    "        # Add the next character to the text\n",
    "        text += next_char\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The function `generate_text` generates a string of the specified length. \n",
    "- It starts with the initial string \"TH\" and generates each subsequent character by using the previous two characters \n",
    "as a prefix. For each prefix, it calls the `weighted_random_choice` function to select the next character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of generated text: 10000\n",
      "Generated text (first 500 characters):\n",
      "THE I CRIES ING THAREPIN TO DO HE KNOTHE UND SHTE GUY WAS COUR EEPLECHICH THIM. HIS TROU ASS FOR FORS. INGENDPIES LASTWETHOLD TO THERBE ANDAY THEY NOTHE DRE GIVERHAT ABOU A BE FORNIONED HE OD QUITHE GIRL BE GET THE THE PENBEF. TO RAT ALL WHE EIT ENEWESTAKEARROJECKEIGHT PLOOT WORRIDE ASUPPER RAD WILL WITHERMORSTATENCE WORE WERIGH HE WASTO UND WITE SUCHER AINDERCH ST RE OF THE AND GOUNT THAT YOUTGUE NE TH AT MES ZAALL ARTHE WED IS WAS HEBLAD DED NONG.THISIT LAR FOUN THAR ATIM HICHES NOT WHY AN KIT\n"
     ]
    }
   ],
   "source": [
    "# Example usage: generate a string of 10,000 characters\n",
    "generated_text = generate_text(combined_trigram_counts)\n",
    "\n",
    "# Check the length of the generated text\n",
    "print(f\"Length of generated text: {len(generated_text)}\")\n",
    "\n",
    "# Display the first 500 characters to verify\n",
    "print(f\"Generated text (first 500 characters):\\n{generated_text[:500]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The string created will not make sense it will be random words eample \"THIM CLOPPINE SIOUND SCE BERREN EN SHERVE SE.\" this is a sample taken from my results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_text(text, output_file):\n",
    "    \"\"\"\n",
    "    Saves the generated text to a file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saves the String to a text file called generatedText.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 20\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Place your text here\"\"\"  # Paste the full text here\n",
    "print(\"Number of characters:\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test to see if it counted the charaters correctly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research of Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 is asking me check the quality of my generated text by analyzing how many of the words are actual english words.\n",
    "\n",
    "1. Get the english word list:\n",
    "    - Get the words.txt from his github and place in my repository\n",
    "\n",
    "2. Split Your Generated Text into Words:\n",
    "    - Use spaces in your generated text to separate it into \"words.\"\n",
    "    - Example, \"THE CAT IS ON THE MAT\" would be split into [\"THE\", \"CAT\", \"IS\", \"ON\", \"THE\", \"MAT\"].\n",
    "\n",
    "3. Check Each Word Against words.txt:\n",
    "    - Load the list of valid English words from words.txt.\n",
    "    - For each \"word\" in your generated text, check if it exists in the list.\n",
    "\n",
    "4. Calculate the Percentage of Real Words:\n",
    "    - Count how many words in the generated text are actual English words.\n",
    "    - Divide this count by the total number of words in the generated text and multiply by 100 to get the percentage of real English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the English words from words.txt\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m english_words \u001b[38;5;241m=\u001b[39m load_word_list(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(english_words)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m English words.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36mload_word_list\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word_list\u001b[39m(file_path):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m         words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines())\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "File \u001b[1;32mc:\\Users\\timmy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'words.txt'"
     ]
    }
   ],
   "source": [
    "def load_word_list(file_path):\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        words = set(file.read().splitlines())\n",
    "    return words\n",
    "\n",
    "# Load the English words from words.txt\n",
    "english_words = load_word_list('data/words.txt')\n",
    "print(f\"Loaded {len(english_words)} English words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the english words from the word.txt file and place into a set\n",
    "- This list will b eused to check if each word is actual a english word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in generated text: 1757\n"
     ]
    }
   ],
   "source": [
    "def split_into_words(text):\n",
    "    \n",
    "    return text.split()\n",
    "\n",
    "# Split the generated text into words\n",
    "words_in_generated_text = split_into_words(generated_text)\n",
    "print(f\"Total words in generated text: {len(words_in_generated_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I split the generated text from the file generated_text into words by using spaces.\n",
    "- Each sequence of characters separated by spaces will be treated as a \"word\" and checked against the English word list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real English words in generated text: 622\n"
     ]
    }
   ],
   "source": [
    "def count_real_words(words, english_words_set):\n",
    "   \n",
    "    real_word_count = sum(1 for word in words if word in english_words_set)\n",
    "    return real_word_count\n",
    "\n",
    "# Count how many words in the generated text are real English words\n",
    "real_word_count = count_real_words(words_in_generated_text, english_words)\n",
    "print(f\"Real English words in generated text: {real_word_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Each word we find we check if it exist on the word.txt file to see if its a real english word and gives us a count of how many words there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of real English words: 35.40%\n"
     ]
    }
   ],
   "source": [
    "def calculate_percentage(real_word_count, total_word_count):\n",
    "    \n",
    "    if total_word_count == 0:\n",
    "        return 0\n",
    "    return (real_word_count / total_word_count) * 100\n",
    "\n",
    "# Calculate the percentage of real English words\n",
    "total_word_count = len(words_in_generated_text)\n",
    "real_word_percentage = calculate_percentage(real_word_count, total_word_count)\n",
    "print(f\"Percentage of real English words: {real_word_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the precentage of real english words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research of Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task 4 is asking me to export my model as a json file using Pythons built in json library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model saved to 'trigrams.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def export_model_to_json(trigram_model, output_file):\n",
    "    \n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(trigram_model, file, indent=4)  # Use indent=4 for readable formatting\n",
    "\n",
    "        # Export the trigram model to a JSON file\n",
    "export_model_to_json(combined_trigram_counts, 'trigrams.json')\n",
    "print(\"Trigram model saved to 'trigrams.json'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the json Library\n",
    "- Will convert the trigram model to a json format and save it to a file \n",
    "- I then call the function to export `combined_trigram_counts` as json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
